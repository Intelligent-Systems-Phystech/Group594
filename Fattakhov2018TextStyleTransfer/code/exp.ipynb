{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "\n",
    "# -- local imports\n",
    "from data_loader import get_corpus_size, build_vocab, preprocess, get_input_queues\n",
    "import layers as lay\n",
    "from decoders import gumbel_decoder_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'data/pan18-style-change-detection-training-dataset-2018-01-31/'\n",
    "train_files = os.listdir(d)\n",
    "\n",
    "truth_files = []\n",
    "text_files = []\n",
    "for i in train_files:\n",
    "    if 'truth' in i:\n",
    "        truth_files += [i]\n",
    "    else:\n",
    "        text_files += [i]\n",
    "        \n",
    "truth_files = np.sort(truth_files)\n",
    "text_files = np.sort(text_files)\n",
    "\n",
    "\n",
    "texts = []\n",
    "truths = []\n",
    "for i, j in zip(text_files, truth_files):\n",
    "    file = open(d+i, 'r') \n",
    "    texts += [file.read()]\n",
    "    \n",
    "    file = open(d+j, 'r') \n",
    "    truths += [file.read()]\n",
    "    \n",
    "df = pd.DataFrame(texts)\n",
    "df.columns = ['text']\n",
    "df['coord'] = truths\n",
    "df['coord'] = df['coord'].apply(lambda x: x.split('[')[1].split(']')[0])\n",
    "\n",
    "\n",
    "d = 'data/pan18-style-change-detection-validation-dataset-2018-01-31/'\n",
    "train_files = os.listdir(d)\n",
    "\n",
    "truth_files = []\n",
    "text_files = []\n",
    "for i in train_files:\n",
    "    if 'truth' in i:\n",
    "        truth_files += [i]\n",
    "    else:\n",
    "        text_files += [i]\n",
    "        \n",
    "truth_files = np.sort(truth_files)\n",
    "text_files = np.sort(text_files)\n",
    "\n",
    "\n",
    "texts = []\n",
    "truths = []\n",
    "for i, j in zip(text_files, truth_files):\n",
    "    file = open(d+i, 'r') \n",
    "    texts += [file.read()]\n",
    "    \n",
    "    file = open(d+j, 'r') \n",
    "    truths += [file.read()]\n",
    "    \n",
    "df_val = pd.DataFrame(texts)\n",
    "df_val.columns = ['text']\n",
    "df_val['coord'] = truths\n",
    "df_val['coord'] = df_val['coord'].apply(lambda x: x.split('[')[1].split(']')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GENERATOR_PREFIX = \"generator\"\n",
    "DISCRIMINATOR_PREFIX = \"discriminator\"\n",
    "\n",
    "GeneratorTuple = namedtuple(\"Generator\",\n",
    "                            [\"rnn_outputs\", \"flat_logits\", \"probs\", \"loss\", \"embedding_matrix\", \"output_projections\"])\n",
    "DiscriminatorTuple = namedtuple(\"Discriminator\",\n",
    "                                [\"rnn_final_state\", \"prediction_logits\", \"loss\"])\n",
    "\n",
    "\n",
    "# TODO: separate the variables for generator and discriminators\n",
    "class Model:\n",
    "    def __init__(self, corpus, **opts):\n",
    "        self.corpus = corpus\n",
    "\n",
    "        self.opts = opts\n",
    "\n",
    "        self.global_step = get_or_create_global_step()\n",
    "        self.increment_global_step_op = tf.assign(self.global_step, self.global_step + 1, name=\"increment_global_step\")\n",
    "\n",
    "        self.corpus_size = get_corpus_size(self.corpus[\"train\"])\n",
    "        self.corpus_size_valid = get_corpus_size(self.corpus[\"valid\"])\n",
    "\n",
    "        self.word2idx, self.idx2word = build_vocab(self.corpus[\"train\"])\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "        self.generator_template = tf.make_template(GENERATOR_PREFIX, generator)\n",
    "        self.discriminator_template = tf.make_template(DISCRIMINATOR_PREFIX, discriminator)\n",
    "\n",
    "        self.enqueue_data, _, source, target, sequence_length = \\\n",
    "            prepare_data(self.corpus[\"train\"], self.word2idx, num_threads=7, **self.opts)\n",
    "\n",
    "        # TODO: option to either do pretrain or just generate?\n",
    "        self.g_tensors_pretrain = self.generator_template(\n",
    "            source, target, sequence_length, self.vocab_size, **self.opts)\n",
    "\n",
    "        self.enqueue_data_valid, self.input_ph, source_valid, target_valid, sequence_length_valid = \\\n",
    "            prepare_data(self.corpus[\"valid\"], self.word2idx, num_threads=1, **self.opts)\n",
    "\n",
    "        self.g_tensors_pretrain_valid = self.generator_template(\n",
    "            source_valid, target_valid, sequence_length_valid, self.vocab_size, **self.opts)\n",
    "\n",
    "        self.decoder_fn = prepare_custom_decoder(\n",
    "            sequence_length, self.g_tensors_pretrain.embedding_matrix, self.g_tensors_pretrain.output_projections)\n",
    "\n",
    "        self.g_tensors_fake = self.generator_template(\n",
    "            source, target, sequence_length, self.vocab_size, decoder_fn=self.decoder_fn, **self.opts)\n",
    "\n",
    "        self.g_tensors_fake_valid = self.generator_template(\n",
    "            source_valid, target_valid, sequence_length_valid, self.vocab_size, decoder_fn=self.decoder_fn, **self.opts)\n",
    "\n",
    "        # TODO: using the rnn outputs from pretraining as \"real\" instead of target embeddings (aka professor forcing)\n",
    "        self.d_tensors_real = self.discriminator_template(\n",
    "            self.g_tensors_pretrain.rnn_outputs, sequence_length, is_real=True, **self.opts)\n",
    "\n",
    "        # TODO: check to see if sequence_length is correct\n",
    "        self.d_tensors_fake = self.discriminator_template(\n",
    "            self.g_tensors_fake.rnn_outputs, None, is_real=False, **self.opts)\n",
    "\n",
    "        self.g_tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=GENERATOR_PREFIX)\n",
    "        self.d_tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=DISCRIMINATOR_PREFIX)\n",
    "\n",
    "\n",
    "def prepare_data(path, word2idx, num_threads=8, **opts):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        enqueue_data, dequeue_batch = get_input_queues(\n",
    "            path, word2idx, batch_size=opts[\"batch_size\"], num_threads=num_threads)\n",
    "        # TODO: put this logic somewhere else\n",
    "        input_ph = tf.placeholder_with_default(dequeue_batch, (None, None))\n",
    "        source, target, sequence_length = preprocess(input_ph)\n",
    "    return enqueue_data, input_ph, source, target, sequence_length\n",
    "\n",
    "\n",
    "def prepare_custom_decoder(sequence_length, embedding_matrix, output_projections):\n",
    "    # TODO: this is brittle, global variables\n",
    "    cell = tf.get_collection(\"rnn_cell\")[0]\n",
    "    encoder_state = cell.zero_state(tf.shape(sequence_length)[0], tf.float32)\n",
    "\n",
    "    # embedding_matrix = tf.get_collection(\"embedding_matrix\")[0]\n",
    "    # output_projections = tf.get_collection(\"output_projections\")[:2]  # TODO: repeated output_projections\n",
    "\n",
    "    maximum_length = tf.reduce_max(sequence_length) + 3\n",
    "\n",
    "    decoder_fn = gumbel_decoder_fn(encoder_state, embedding_matrix, output_projections, maximum_length)\n",
    "    return decoder_fn\n",
    "\n",
    "\n",
    "def generator(source, target, sequence_length, vocab_size, decoder_fn=None, **opts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: TensorFlow queue or placeholder tensor for word ids for source \n",
    "        target: TensorFlow queue or placeholder tensor for word ids for target\n",
    "        sequence_length: TensorFlow queue or placeholder tensor for number of word ids for each sentence\n",
    "        vocab_size: max vocab size determined from data\n",
    "        decoder_fn: if using custom decoder_fn else use the default dynamic_rnn\n",
    "    \"\"\"\n",
    "    tf.logging.info(\" Setting up generator\")\n",
    "\n",
    "    embedding_layer = lay.embedding_layer(vocab_size, opts[\"embedding_dim\"], name=\"embedding_matrix\")\n",
    "\n",
    "    # TODO: add batch norm?\n",
    "    rnn_outputs = (\n",
    "        source >>\n",
    "        embedding_layer >>\n",
    "        lay.word_dropout_layer(keep_prob=opts[\"word_dropout_keep_prob\"]) >>\n",
    "        lay.recurrent_layer(hidden_dims=opts[\"rnn_hidden_dim\"], keep_prob=opts[\"recurrent_dropout_keep_prob\"],\n",
    "                            sequence_length=sequence_length, decoder_fn=decoder_fn, name=\"rnn_cell\")\n",
    "    )\n",
    "\n",
    "    output_projection_layer = lay.dense_layer(hidden_dims=vocab_size, name=\"output_projections\")\n",
    "\n",
    "    flat_logits = (\n",
    "        rnn_outputs >>\n",
    "        lay.reshape_layer(shape=(-1, opts[\"rnn_hidden_dim\"])) >>\n",
    "        output_projection_layer\n",
    "    )\n",
    "\n",
    "    probs = flat_logits >> lay.softmax_layer()\n",
    "\n",
    "    embedding_matrix = embedding_layer.get_variables_in_scope()\n",
    "    output_projections = output_projection_layer.get_variables_in_scope()\n",
    "\n",
    "    if decoder_fn is not None:\n",
    "        return GeneratorTuple(rnn_outputs=rnn_outputs, flat_logits=flat_logits, probs=probs, loss=None,\n",
    "                              embedding_matrix=embedding_matrix[0], output_projections=output_projections)\n",
    "\n",
    "    loss = (\n",
    "        flat_logits >>\n",
    "        lay.cross_entropy_layer(target=target) >>\n",
    "        lay.reshape_layer(shape=tf.shape(target)) >>\n",
    "        lay.mean_loss_by_example_layer(sequence_length=sequence_length)\n",
    "    )\n",
    "\n",
    "    # TODO: add dropout penalty\n",
    "    return GeneratorTuple(rnn_outputs=rnn_outputs, flat_logits=flat_logits, probs=probs, loss=loss,\n",
    "                          embedding_matrix=embedding_matrix[0], output_projections=output_projections)\n",
    "\n",
    "\n",
    "def discriminator(input_vectors, sequence_length, is_real=True, **opts):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_vectors: output of the RNN either from real or generated data\n",
    "        sequence_length: TensorFlow queue or placeholder tensor for number of word ids for each sentence\n",
    "        is_real: if True, RNN outputs when feeding in actual data, if False feeds in generated data\n",
    "    \"\"\"\n",
    "    tf.logging.info(\" Setting up discriminator\")\n",
    "\n",
    "    rnn_final_state = (\n",
    "        input_vectors >>\n",
    "        lay.dense_layer(hidden_dims=opts[\"embedding_dim\"]) >>\n",
    "        lay.recurrent_layer(sequence_length=sequence_length, hidden_dims=opts[\"rnn_hidden_dim\"],\n",
    "                            return_final_state=True)\n",
    "    )\n",
    "\n",
    "    prediction_logits = (\n",
    "        rnn_final_state >>\n",
    "        lay.dense_layer(hidden_dims=opts[\"output_hidden_dim\"]) >>\n",
    "        lay.relu_layer() >>\n",
    "        lay.dropout_layer(opts[\"output_dropout_keep_prob\"]) >>\n",
    "        lay.dense_layer(hidden_dims=opts[\"output_hidden_dim\"]) >>\n",
    "        lay.relu_layer() >>\n",
    "        lay.dropout_layer(opts[\"output_dropout_keep_prob\"]) >>\n",
    "        lay.dense_layer(hidden_dims=1)\n",
    "    )\n",
    "\n",
    "    if is_real:\n",
    "        target = tf.ones_like(prediction_logits)\n",
    "    else:\n",
    "        target = tf.zeros_like(prediction_logits)\n",
    "\n",
    "    # TODO: add accuracy\n",
    "    loss = (\n",
    "        prediction_logits >>\n",
    "        lay.sigmoid_cross_entropy_layer(target=target)\n",
    "    )\n",
    "\n",
    "    # TODO: return logits in case for WGAN and l2 GANs\n",
    "    return DiscriminatorTuple(rnn_final_state=rnn_final_state, prediction_logits=prediction_logits, loss=loss)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from data_loader import DATA_PATH\n",
    "    from train import opts\n",
    "\n",
    "    corpus = DATA_PATH[\"ptb\"]\n",
    "\n",
    "    model = Model(corpus, **opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
